{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# ===== 1. DỮ LIỆU =====\n",
        "chars = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "vocab = ['<pad>', '<sos>', '<eos>'] + chars\n",
        "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "def random_string(min_len=3, max_len=6):\n",
        "    L = random.randint(min_len, max_len)\n",
        "    return ''.join(random.choices(chars, k=L))\n",
        "\n",
        "def encode(seq):\n",
        "    return [stoi['<sos>']] + [stoi[c] for c in seq] + [stoi['<eos>']]\n",
        "\n",
        "def pad_batch(seqs):\n",
        "    max_len = max(len(s) for s in seqs)\n",
        "    pad_idx = stoi['<pad>']\n",
        "    padded = [s + [pad_idx]*(max_len-len(s)) for s in seqs]\n",
        "    mask = [[1 if t != pad_idx else 0 for t in s] for s in padded]\n",
        "    return torch.tensor(padded), torch.tensor(mask)\n",
        "\n",
        "# Tạo dữ liệu train\n",
        "data = [random_string() for _ in range(3000)]\n",
        "inputs = [encode(s) for s in data]\n",
        "targets = [encode(s[::-1]) for s in data]\n",
        "\n",
        "\n",
        "# ===== 2. MÔ HÌNH =====\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        emb = self.embed(x)\n",
        "        lengths = mask.sum(1).cpu()\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths, batch_first=True, enforce_sorted=False)\n",
        "        _, hidden = self.rnn(packed)\n",
        "        return hidden  # (1, batch, hidden)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, hidden):\n",
        "        emb = self.embed(tgt)\n",
        "        out, _ = self.rnn(emb, hidden)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size, hidden_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim)\n",
        "\n",
        "    def forward(self, src, src_mask, tgt):\n",
        "        hidden = self.encoder(src, src_mask)\n",
        "        logits = self.decoder(tgt[:, :-1], hidden)  # bỏ token <eos>\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ===== 3. HUẤN LUYỆN =====\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = Seq2Seq(len(vocab), 64).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=stoi['<pad>'])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "for epoch in range(200):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    # shuffle dữ liệu\n",
        "    order = torch.randperm(len(inputs))\n",
        "    for i in range(0, len(inputs), batch_size):\n",
        "        idx = order[i:i+batch_size]\n",
        "        src_batch = [inputs[j] for j in idx]\n",
        "        tgt_batch = [targets[j] for j in idx]\n",
        "        src, src_mask = pad_batch(src_batch)\n",
        "        tgt, _ = pad_batch(tgt_batch)\n",
        "        src, src_mask, tgt = src.to(device), src_mask.to(device), tgt.to(device)\n",
        "\n",
        "        logits = model(src, src_mask, tgt)\n",
        "        loss = criterion(logits.reshape(-1, len(vocab)), tgt[:, 1:].reshape(-1))\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d}, loss={total_loss/len(inputs):.4f}\")\n",
        "\n",
        "# ===== 4. SUY LUẬN (INFERENCE) =====\n",
        "def predict(seq):\n",
        "    model.eval()\n",
        "    src = torch.tensor([encode(seq)], device=device)\n",
        "    src_mask = torch.ones_like(src)\n",
        "    hidden = model.encoder(src, src_mask)\n",
        "\n",
        "    dec_input = torch.tensor([[stoi['<sos>']]], device=device)\n",
        "    result = ''\n",
        "    for _ in range(10):\n",
        "        logits = model.decoder(dec_input, hidden)\n",
        "        next_token = logits[:, -1].argmax(1).item()\n",
        "        ch = itos[next_token]\n",
        "        if ch == '<eos>': break\n",
        "        result += ch\n",
        "        dec_input = torch.cat([dec_input, torch.tensor([[next_token]], device=device)], dim=1)\n",
        "    return result\n",
        "\n",
        "print(\"\\n=== TEST ===\")\n",
        "for s in [\"abc\", \"hello\", \"xyz\", \"chat\"]:\n",
        "    print(f\"{s} -> {predict(s)}\")\n",
        "\n",
        "# ===== 5. LƯU MÔ HÌNH =====\n",
        "torch.save(model.state_dict(), \"seq2seq_reverse.pt\")\n",
        "print(\"Đã lưu mô hình vào file seq2seq_reverse.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6h9xBRELeK-",
        "outputId": "71683af1-3538-489d-af66-de603de27039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001, loss=0.0486\n",
            "Epoch 002, loss=0.0409\n",
            "Epoch 003, loss=0.0340\n",
            "Epoch 004, loss=0.0286\n",
            "Epoch 005, loss=0.0242\n",
            "Epoch 006, loss=0.0204\n",
            "Epoch 007, loss=0.0171\n",
            "Epoch 008, loss=0.0144\n",
            "Epoch 009, loss=0.0122\n",
            "Epoch 010, loss=0.0104\n",
            "Epoch 011, loss=0.0089\n",
            "Epoch 012, loss=0.0077\n",
            "Epoch 013, loss=0.0067\n",
            "Epoch 014, loss=0.0059\n",
            "Epoch 015, loss=0.0051\n",
            "Epoch 016, loss=0.0046\n",
            "Epoch 017, loss=0.0041\n",
            "Epoch 018, loss=0.0037\n",
            "Epoch 019, loss=0.0032\n",
            "Epoch 020, loss=0.0030\n",
            "Epoch 021, loss=0.0026\n",
            "Epoch 022, loss=0.0024\n",
            "Epoch 023, loss=0.0023\n",
            "Epoch 024, loss=0.0021\n",
            "Epoch 025, loss=0.0020\n",
            "Epoch 026, loss=0.0018\n",
            "Epoch 027, loss=0.0017\n",
            "Epoch 028, loss=0.0015\n",
            "Epoch 029, loss=0.0013\n",
            "Epoch 030, loss=0.0013\n",
            "Epoch 031, loss=0.0012\n",
            "Epoch 032, loss=0.0011\n",
            "Epoch 033, loss=0.0011\n",
            "Epoch 034, loss=0.0010\n",
            "Epoch 035, loss=0.0010\n",
            "Epoch 036, loss=0.0011\n",
            "Epoch 037, loss=0.0011\n",
            "Epoch 038, loss=0.0010\n",
            "Epoch 039, loss=0.0009\n",
            "Epoch 040, loss=0.0008\n",
            "Epoch 041, loss=0.0007\n",
            "Epoch 042, loss=0.0006\n",
            "Epoch 043, loss=0.0008\n",
            "Epoch 044, loss=0.0008\n",
            "Epoch 045, loss=0.0008\n",
            "Epoch 046, loss=0.0009\n",
            "Epoch 047, loss=0.0007\n",
            "Epoch 048, loss=0.0006\n",
            "Epoch 049, loss=0.0005\n",
            "Epoch 050, loss=0.0004\n",
            "Epoch 051, loss=0.0004\n",
            "Epoch 052, loss=0.0003\n",
            "Epoch 053, loss=0.0003\n",
            "Epoch 054, loss=0.0003\n",
            "Epoch 055, loss=0.0003\n",
            "Epoch 056, loss=0.0002\n",
            "Epoch 057, loss=0.0002\n",
            "Epoch 058, loss=0.0002\n",
            "Epoch 059, loss=0.0002\n",
            "Epoch 060, loss=0.0002\n",
            "Epoch 061, loss=0.0002\n",
            "Epoch 062, loss=0.0002\n",
            "Epoch 063, loss=0.0002\n",
            "Epoch 064, loss=0.0002\n",
            "Epoch 065, loss=0.0003\n",
            "Epoch 066, loss=0.0022\n",
            "Epoch 067, loss=0.0021\n",
            "Epoch 068, loss=0.0011\n",
            "Epoch 069, loss=0.0007\n",
            "Epoch 070, loss=0.0004\n",
            "Epoch 071, loss=0.0003\n",
            "Epoch 072, loss=0.0003\n",
            "Epoch 073, loss=0.0002\n",
            "Epoch 074, loss=0.0002\n",
            "Epoch 075, loss=0.0002\n",
            "Epoch 076, loss=0.0002\n",
            "Epoch 077, loss=0.0002\n",
            "Epoch 078, loss=0.0001\n",
            "Epoch 079, loss=0.0001\n",
            "Epoch 080, loss=0.0001\n",
            "Epoch 081, loss=0.0001\n",
            "Epoch 082, loss=0.0001\n",
            "Epoch 083, loss=0.0001\n",
            "Epoch 084, loss=0.0001\n",
            "Epoch 085, loss=0.0001\n",
            "Epoch 086, loss=0.0001\n",
            "Epoch 087, loss=0.0001\n",
            "Epoch 088, loss=0.0001\n",
            "Epoch 089, loss=0.0001\n",
            "Epoch 090, loss=0.0001\n",
            "Epoch 091, loss=0.0001\n",
            "Epoch 092, loss=0.0001\n",
            "Epoch 093, loss=0.0001\n",
            "Epoch 094, loss=0.0001\n",
            "Epoch 095, loss=0.0001\n",
            "Epoch 096, loss=0.0001\n",
            "Epoch 097, loss=0.0001\n",
            "Epoch 098, loss=0.0001\n",
            "Epoch 099, loss=0.0001\n",
            "Epoch 100, loss=0.0001\n",
            "Epoch 101, loss=0.0001\n",
            "Epoch 102, loss=0.0001\n",
            "Epoch 103, loss=0.0001\n",
            "Epoch 104, loss=0.0001\n",
            "Epoch 105, loss=0.0001\n",
            "Epoch 106, loss=0.0001\n",
            "Epoch 107, loss=0.0001\n",
            "Epoch 108, loss=0.0000\n",
            "Epoch 109, loss=0.0000\n",
            "Epoch 110, loss=0.0000\n",
            "Epoch 111, loss=0.0000\n",
            "Epoch 112, loss=0.0000\n",
            "Epoch 113, loss=0.0000\n",
            "Epoch 114, loss=0.0000\n",
            "Epoch 115, loss=0.0000\n",
            "Epoch 116, loss=0.0000\n",
            "Epoch 117, loss=0.0000\n",
            "Epoch 118, loss=0.0000\n",
            "Epoch 119, loss=0.0000\n",
            "Epoch 120, loss=0.0000\n",
            "Epoch 121, loss=0.0000\n",
            "Epoch 122, loss=0.0000\n",
            "Epoch 123, loss=0.0000\n",
            "Epoch 124, loss=0.0000\n",
            "Epoch 125, loss=0.0000\n",
            "Epoch 126, loss=0.0000\n",
            "Epoch 127, loss=0.0000\n",
            "Epoch 128, loss=0.0000\n",
            "Epoch 129, loss=0.0000\n",
            "Epoch 130, loss=0.0000\n",
            "Epoch 131, loss=0.0043\n",
            "Epoch 132, loss=0.0044\n",
            "Epoch 133, loss=0.0016\n",
            "Epoch 134, loss=0.0008\n",
            "Epoch 135, loss=0.0005\n",
            "Epoch 136, loss=0.0003\n",
            "Epoch 137, loss=0.0002\n",
            "Epoch 138, loss=0.0002\n",
            "Epoch 139, loss=0.0001\n",
            "Epoch 140, loss=0.0001\n",
            "Epoch 141, loss=0.0001\n",
            "Epoch 142, loss=0.0001\n",
            "Epoch 143, loss=0.0001\n",
            "Epoch 144, loss=0.0001\n",
            "Epoch 145, loss=0.0001\n",
            "Epoch 146, loss=0.0001\n",
            "Epoch 147, loss=0.0001\n",
            "Epoch 148, loss=0.0001\n",
            "Epoch 149, loss=0.0001\n",
            "Epoch 150, loss=0.0001\n",
            "Epoch 151, loss=0.0001\n",
            "Epoch 152, loss=0.0001\n",
            "Epoch 153, loss=0.0000\n",
            "Epoch 154, loss=0.0000\n",
            "Epoch 155, loss=0.0000\n",
            "Epoch 156, loss=0.0000\n",
            "Epoch 157, loss=0.0000\n",
            "Epoch 158, loss=0.0000\n",
            "Epoch 159, loss=0.0000\n",
            "Epoch 160, loss=0.0000\n",
            "Epoch 161, loss=0.0000\n",
            "Epoch 162, loss=0.0000\n",
            "Epoch 163, loss=0.0000\n",
            "Epoch 164, loss=0.0000\n",
            "Epoch 165, loss=0.0000\n",
            "Epoch 166, loss=0.0000\n",
            "Epoch 167, loss=0.0000\n",
            "Epoch 168, loss=0.0000\n",
            "Epoch 169, loss=0.0000\n",
            "Epoch 170, loss=0.0000\n",
            "Epoch 171, loss=0.0000\n",
            "Epoch 172, loss=0.0000\n",
            "Epoch 173, loss=0.0000\n",
            "Epoch 174, loss=0.0000\n",
            "Epoch 175, loss=0.0000\n",
            "Epoch 176, loss=0.0000\n",
            "Epoch 177, loss=0.0000\n",
            "Epoch 178, loss=0.0000\n",
            "Epoch 179, loss=0.0000\n",
            "Epoch 180, loss=0.0000\n",
            "Epoch 181, loss=0.0000\n",
            "Epoch 182, loss=0.0000\n",
            "Epoch 183, loss=0.0000\n",
            "Epoch 184, loss=0.0000\n",
            "Epoch 185, loss=0.0000\n",
            "Epoch 186, loss=0.0000\n",
            "Epoch 187, loss=0.0000\n",
            "Epoch 188, loss=0.0000\n",
            "Epoch 189, loss=0.0000\n",
            "Epoch 190, loss=0.0000\n",
            "Epoch 191, loss=0.0000\n",
            "Epoch 192, loss=0.0000\n",
            "Epoch 193, loss=0.0000\n",
            "Epoch 194, loss=0.0000\n",
            "Epoch 195, loss=0.0000\n",
            "Epoch 196, loss=0.0000\n",
            "Epoch 197, loss=0.0000\n",
            "Epoch 198, loss=0.0000\n",
            "Epoch 199, loss=0.0000\n",
            "Epoch 200, loss=0.0000\n",
            "\n",
            "=== TEST ===\n",
            "abc -> cba\n",
            "hello -> olelh\n",
            "xyz -> zyx\n",
            "chat -> tahc\n",
            "Đã lưu mô hình vào file seq2seq_reverse.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 6. TẢI LẠI MÔ HÌNH (BỎ QUA TRAIN) =====\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = Seq2Seq(len(vocab), 64).to(device)\n",
        "model.load_state_dict(torch.load(\"seq2seq_reverse.pt\", map_location=device))\n",
        "model.eval()\n",
        "print(\"Đã tải mô hình huấn luyện sẵn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpxZJUoMMGY7",
        "outputId": "31ca506a-5301-4fbf-de7a-a70cef0ed487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đã tải mô hình huấn luyện sẵn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict('pyth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nNPRPTCTOLo2",
        "outputId": "bd297b47-fd27-4530-a56b-942a14725848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'htyp'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3OQ4mxcyXxLH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}